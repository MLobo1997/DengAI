{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city',\n",
       " 'year',\n",
       " 'weekofyear',\n",
       " 'week_start_date',\n",
       " 'ndvi_ne',\n",
       " 'ndvi_nw',\n",
       " 'ndvi_se',\n",
       " 'ndvi_sw',\n",
       " 'precipitation_amt_mm',\n",
       " 'reanalysis_air_temp_k',\n",
       " 'reanalysis_avg_temp_k',\n",
       " 'reanalysis_dew_point_temp_k',\n",
       " 'reanalysis_max_air_temp_k',\n",
       " 'reanalysis_min_air_temp_k',\n",
       " 'reanalysis_precip_amt_kg_per_m2',\n",
       " 'reanalysis_relative_humidity_percent',\n",
       " 'reanalysis_sat_precip_amt_mm',\n",
       " 'reanalysis_specific_humidity_g_per_kg',\n",
       " 'reanalysis_tdtr_k',\n",
       " 'station_avg_temp_c',\n",
       " 'station_diur_temp_rng_c',\n",
       " 'station_max_temp_c',\n",
       " 'station_min_temp_c',\n",
       " 'station_precip_mm']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1 = pd.read_csv('data/dengue_features_train.csv')\n",
    "y_train = pd.read_csv('data/dengue_labels_train.csv')['total_cases']\n",
    "attr = list(X_train_1)\n",
    "attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the noisy training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1451, 24)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bools_to_indexes(booleans):\n",
    "    r = []\n",
    "    for idx, x in enumerate(booleans):\n",
    "        if x:\n",
    "            r.append(idx)\n",
    "    return r\n",
    "\n",
    "idx = bools_to_indexes(X_train_1['weekofyear'] == 53)\n",
    "y_train.drop(idx, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_train_1.drop(idx, inplace=True)\n",
    "X_train_1.reset_index(drop=True, inplace=True)\n",
    "X_train_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "score_metric='neg_mean_absolute_error'\n",
    "jobs=-1 #-1 to make it execute in parallel\n",
    "verbose_level = 0\n",
    "random_n = 42\n",
    "base_args = {'estimator': None, 'param_distributions': None, 'n_iter': None, 'scoring': score_metric, 'n_jobs': jobs, 'cv': None, 'verbose': verbose_level, 'random_state': random_n, 'return_train_score': True, 'iid': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR\n",
    "* The results with the kernel *sigmoid* and *poly* were too bad, so we removed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds=10\n",
    "n_iter_search = 20\n",
    "C = sp_randint(0, 10000)\n",
    "params = {'kernel':['linear'], 'gamma':['scale'], 'C': C}\n",
    "SVR_optimizer = RandomizedSearchCV(estimator=SVR(), param_distributions=params, n_iter=n_iter_search, scoring=score_metric, n_jobs=jobs, cv=k_folds, verbose=verbose_level, random_state=random_n, return_train_score=True, iid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "* 18.01 - with 2 previous weeks & without PCA & with (max_depth=6, min_samples_leaf=0.1611807565247405, min_samples_split=0.11193019906931466)\n",
    "* 18.29 - With PCA at 0.9\n",
    "* 18.27 - With PCA at 0.95\n",
    "* 18.36 - With PCA at 0.65. PCA appears to be only making the model worse.\n",
    "* 18.38 - Without PCA and with previous weeks. Clearly the previous weeks are useful\n",
    "* 17.87 - Without PCA and with 3 previous weeks\n",
    "* 17.86 - Without PCA and with 4 previous weeks\n",
    "* 18.28 - With PCA 0.95 and 3 previous weeks fixed\n",
    "* 9.16 - Without PCA, with 3 weeks and 1 last infection (max_depth=5, min_samples_leaf=0.03, min_samples_split=0.108)\n",
    "* **9.04** - Without PCA, with 3 weeks and 1 last infection (max_depth=5, min_samples_leaf=0.03, min_samples_split=0.108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds=10\n",
    "n_iter_search = 100\n",
    "min_samples = sp_uniform(0.01, 0.35)\n",
    "params = {'criterion':['mae'], 'max_depth': sp_randint(2, 10), 'min_samples_split': min_samples, 'min_samples_leaf': min_samples}\n",
    "Tree_optimizer = RandomizedSearchCV(estimator=DecisionTreeRegressor(), param_distributions=params, n_iter=n_iter_search, scoring=score_metric, n_jobs=jobs, cv=k_folds, verbose=verbose_level, random_state=random_n, return_train_score=True, iid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "* 18.34 With 4 previous weeks and without PCA\n",
    "* 17.79 With fixed 3 previous weeks and PCA at 0.95 (n_estimators= ?, max_depth = 2, min_samples_leaf=0.112, min_samples_split=0.224)\n",
    "* 17.74 With fixed 3 previous weeks and without PCA (n_estimators= 13 max_depth = 5, min_samples_leaf=0.09, min_samples_split=0.24)\n",
    "* **9.13** with 3 previous weeks and 1 last infected (n_estimators=9 max_depth = 9, min_samples_leaf=0.014, min_samples_split=0.07)\n",
    "* 9.22 with 3 previous weeks and 3 last infected (n_estimators=9 max_depth = 9, min_samples_leaf=0.014, min_samples_split=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds=10\n",
    "n_iter_search = 40\n",
    "params = {'n_estimators': sp_randint(2,50), 'criterion':['mae'], 'max_depth': sp_randint(2, 10)}\n",
    "Forest_optimizer = RandomizedSearchCV(estimator=RandomForestRegressor(n_jobs=-1), param_distributions=params, n_iter=n_iter_search, scoring=score_metric, n_jobs=jobs, cv=k_folds, verbose=verbose_level, random_state=random_n, return_train_score=True, iid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost of Trees\n",
    "* 10.78 - With 3 last weeks a 3 last infected \n",
    "* **8.49** - With 3 last weeks a 3 last infected and only max_depth tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds=10\n",
    "n_iter_search = 20\n",
    "params = {'n_estimators': sp_randint(40, 100), 'base_estimator__criterion':['mae'], 'base_estimator__max_depth': sp_randint(2,7)}\n",
    "AdaTree_optimizer = RandomizedSearchCV(estimator=AdaBoostRegressor(base_estimator=DecisionTreeRegressor()), param_distributions=params, n_iter=n_iter_search, scoring=score_metric, n_jobs=jobs, cv=k_folds, verbose=verbose_level, random_state=random_n, return_train_score=True, iid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "* 21.349 - with PCA at 0.65 & 2 previous weeks\n",
    "* 20.36  - without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds=10\n",
    "n_iter_search = 100\n",
    "params = {'n_neighbors': sp_randint(3,150), 'weights': ['uniform', 'distance']}\n",
    "KNN_optimizer = RandomizedSearchCV(estimator=KNeighborsRegressor(n_jobs=-1), param_distributions=params, n_iter=n_iter_search, scoring=score_metric, n_jobs=jobs, cv=k_folds, verbose=verbose_level, random_state=random_n, return_train_score=True, iid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "* Interestingly, PCA makes all the models worst in this case.\n",
    "* After the exaustive search, the best model was the SVR which obtained an MAE of 6.52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score of 10.757898351648352 with the estimator DecisionTreeRegressor(criterion='mae', max_depth=7, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=0.048518173584686866,\n",
      "           min_samples_split=0.08977730688967958,\n",
      "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "           splitter='best')\n",
      "1/4\t\n",
      "Best score of 8.57545699492815 with the estimator RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=5,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=26, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "2/4\t3/4\t4/4\t"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from OurPipeline import create_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "optimizers=[Tree_optimizer, Forest_optimizer, AdaTree_optimizer, KNN_optimizer]#, SVR_optimizer]\n",
    "weeks = [1]\n",
    "weeks_infected = [3]\n",
    "pca = [None]\n",
    "\n",
    "n_total = len(optimizers) * len(weeks) * len(weeks_infected) * len(pca)\n",
    "\n",
    "results=[]\n",
    "best_attempt = None\n",
    "best_score = np.inf\n",
    "idx=0\n",
    "for opt in optimizers:\n",
    "    for w in weeks:\n",
    "        for wi in weeks_infected:\n",
    "            for p in pca:\n",
    "                pipeline = create_pipeline(attr, n_weeks=w, n_weeks_infected=wi, estimator_optimizer=opt, add_noise=True, noise_mean=6.5, noise_std=6.5, pca=None)\n",
    "                pipeline.fit(X_train_1, y_train)\n",
    "                score = pipeline.named_steps['est_opt'].best_score_\n",
    "                best_estimator = pipeline.named_steps['est_opt'].best_estimator_\n",
    "                attempt = [best_estimator, w, wi, p, score]\n",
    "                if abs(score) < best_score:\n",
    "                    best_score = abs(score)\n",
    "                    best_attempt = attempt\n",
    "                    print('\\nBest score of {} with the estimator {}'.format(best_score, best_estimator))\n",
    "                idx+=1\n",
    "                print(str(idx) + '/' + str(n_total), end='\\t')\n",
    "                results.append(attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results, columns=['estimator', 'weeks', 'weeks_infected', 'PCA', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SVR(C=5191, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
       "   kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       " 1,\n",
       " 3,\n",
       " None,\n",
       " -6.522347109745663]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from OurPipeline import create_pipeline\n",
    "\n",
    "pipeline = create_pipeline(attr, n_weeks=1, n_weeks_infected=3, pca=None)\n",
    "X_train = pipeline.fit_transform(X_train_1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=5191, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
       "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVR(kernel= 'linear', C=5191, gamma='scale')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 24)\n"
     ]
    }
   ],
   "source": [
    "X_test_1 = pd.read_csv('data/dengue_features_test.csv')\n",
    "print(X_test_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One by one prediction\n",
    "* Given that we are making sequential predictions, i.e.: the prediction from a week relies on the prediction from the previous weeks, we must make the transformations and predictions one by one.\n",
    "* Given that this kind of prediction is very prone to a snowball effect on errors our first solution had an error of 26. To solve this we came up with the idea of adding noise to the train data. However for this solution we need to know both: the mean of the error and its standard deviation (*std*). We already know the mean (MAE), we just need to know the *std*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=[]\n",
    "for idx in range(X_test_1.shape[0]):\n",
    "    x = pipeline.transform(X_test_1.loc[idx:idx,:])\n",
    "    pred = model.predict(x)\n",
    "    pred = int(np.round(pred))\n",
    "    pipeline.named_steps['l_infected'].append_y(pred)\n",
    "    predictions.append(pred)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating an approximation of the *std*\n",
    "* It is approximately 10.9. We can see that the MAE is close to the one calculated in the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from OurPipeline import create_pipeline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "pipeline = create_pipeline(attr, n_weeks=1, n_weeks_infected=3, pca=None)\n",
    "X_train = pipeline.fit_transform(X_train_1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = ShuffleSplit(n_splits=1, train_size=1000, test_size=None, random_state=random_n)\n",
    "for train, test in sp.split(X_train, y_train):\n",
    "    X_train_std = X_train[train]\n",
    "    y_train_std = y_train[train]\n",
    "    X_test_std = X_train[test]\n",
    "    y_test_std = y_train[test]\n",
    "X_train_std.shape, y_train_std.shape\n",
    "X_test_std.shape, y_test_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=5191, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
       "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVR(kernel= 'linear', C=5191, gamma='scale')\n",
    "model.fit(X_train_std, y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.7785087719298245, 10.959317651673116)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_std)\n",
    "predictions = list(map(lambda x: int(np.round(x)), predictions))\n",
    "errors = list(map(abs, predictions - y_test_std))\n",
    "np.mean(errors), np.std(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One by one prediction with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from OurPipeline import create_pipeline\n",
    "\n",
    "pipeline = create_pipeline(attr, n_weeks=1, n_weeks_infected=3, add_noise=True, noise_mean=6.5, noise_std=6.5, pca=None)\n",
    "X_train = pipeline.fit_transform(X_train_1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.436222184081323"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVR_optimizer.fit(X_train, y_train)\n",
    "model=SVR_optimizer.best_estimator_\n",
    "SVR_optimizer.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1685, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
       "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=[]\n",
    "for idx in range(X_test_1.shape[0]):\n",
    "    x = pipeline.transform(X_test_1.loc[idx:idx,:])\n",
    "    pred = model.predict(x)\n",
    "    pred = int(np.round(pred))\n",
    "    pipeline.named_steps['l_infected'].append_y(pred)\n",
    "    predictions.append(pred)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(predictions, columns=['total_cases'])\n",
    "x_3 = X_test_1.iloc[:,:3].copy()\n",
    "submit = pd.concat([x_3, submit], axis=1)\n",
    "submit.to_csv('data/submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
